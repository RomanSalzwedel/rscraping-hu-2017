str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
str_dup(char.vec, 3)
length.char.vec <- str_length(char.vec)
char.vec
length.char.vec <- str_length(char.vec)
length.char.vec
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
?str_trim
str_c("text", "manipulation", sep = " ")
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c(char.vec, collapse = "\n")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
?agrep
agrepl("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
agrepl("Donald Trump", "Barack Obama", max.distance = list(all = 3))
library(stringi)
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[humboldt]")
browseURL("https://www.nytimes.com/")
browseURL("http://flukeout.github.io/") # let's play this together until plate 8 or so!
source("packages.r")
email <- "chunkylover53[at]aol[dot]com"
email_new <- email %>% str_replace("\\[at\\]", "@") %>% str_replace("\\[dot\\]", ".")
email_new
str_extract(email_new, "[:digit:]+")
regex <- ".*"
string <- c("1. This is an example string by", "2. Eddie (born 1961 in München)", "!§%$&/)(}")
str_extract_all(string, regex)
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
(name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}")))
(name_sorted <-  str_replace(name, "(.+), (.+)", "\\2 \\1"))
has_title <- str_detect(name, pattern="Dr\\. |Rev\\.")
has_title
name_elements <- str_count(name_sorted, "\\w+")
has_second_name <- ifelse(has_title == FALSE & name_elements > 2, TRUE,
ifelse(has_title == TRUE & name_elements > 3, TRUE, FALSE))
has_second_name
string <- "<title>+++BREAKING NEWS+++</title>"
str_extract(string, "<.+>")
str_extract(string, "<.+?>")
string <- "(5-3)^2=5^2-2*5*3+3^2 conforms to the binomial theorem"
regex <- "[^0-9=+*()]+"
str_extract_all(string, regex)
regex_correct <- "[[:digit:][:punct:]]+"
regex_correct_alt <- "[[:digit:]()=*-^]+"
regex_correct
str_extract_all(string, regex_correct)
str_extract_all(string, regex_correct_alt)
secret <- "clcopCow1zmstc0d87wnkig7OvdicpNuggvhryn92Gjuwczi8hqrfpRxs5Aj5dwpn0TanwoUwisdij7Lj8kpf03AT5Idr3coc0bt7yczjatOaootj55t3Nj3ne6c4Sfek.r1w1YwwojigOd6vrfUrbz2.2bkAnbhzgv4R9i05zEcrop.wAgnb.SqoU65fPa1otfb7wEm24k6t3sR9zqe5fy89n6Nd5t9kc4fE905gmc4Rgxo5nhDk!gr"
(solved <- unlist(str_extract_all(secret, "[[:upper:][:punct:]]")))
str_c(solved, collapse="")
browseURL("https://www.jstatsoft.org/about/editorialTeam")
browseURL("http://www.whoishostingthis.com/tools/user-agent/")
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session("http://www.google.com", user_agent(uastring))
search <- html_form(session)[[1]]
library(rvest)
url <- "http://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
# clean up
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[-1,]
mps <- mps[!is.na(as.character(mps$party)),]
nrow(mps)
# look for Sirs
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir ")
table(mps$party, mps$sir)
prop.table(table(mps$party, mps$sir), 1)
library(stringr)
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir ")
table(mps$party, mps$sir)
mps <- tables[[4]]
head(mps)
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[-1,]
tables <- html_table(url_parsed, fill = FALSE)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
mps <- filter(mps, !str_detect("[edit"))
mps <- filter(mps, !str_detect(X2, "[edit"))
mps <- mps[!str_detect(mps$X1, "[edit"),]
mps <- mps[!str_detect(mps$X1, "\\[edit"),]
nrow(mps)
# look for Sirs
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir ")
table(mps$party, mps$sir)
prop.table(table(mps$party, mps$sir), 1)
mps$name <- as.character(mps$name)
head(mps)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[!str_detect(mps$X1, "\\[edit"),]
mps <- mps[-1,]
nrow(mps)
str_detect(mps$X1, "\\[edit")
str_detect(mps$X1, "\\[edit")
head(mps)
names(mps) <- c("con", "name", "party")
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
# clean up
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[str_detect(mps$con, "\\[edit"),]
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
# clean up
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[!str_detect(mps$con, "\\[edit"),]
head(mps)
mps <- mps[-1,]
nrow(mps)
# look for Sirs
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir ")
table(mps$party, mps$sir)
prop.table(table(mps$party, mps$sir), 1)
812.76*0.94195
(7.50+63.35)*0.94195
(7.50+63.35)*0.91466
765.58+580.07+64.8
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl list")
system("launchctl list")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl list")
system("launchctl unload ~/Library/LaunchAgents/scraperspiegelonline.plist")
system("launchctl stop scraperspiegelonline")
system("launchctl unload ~/Library/LaunchAgents/scraper_spiegel_online.plist")
system("launchctl list")
library(stringr)
library(magrittr)
library(httr)
Sys.time()
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
datetime
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl list")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
source("00-course-setup.r")
source("packages.r")
source("packages.r")
source("functions.r")
starships <- get_all_starships()
library(rwars)
starships <- get_all_starships()
foo <- plyr::rbind.fill(starships$results)
starships$results
foo <- lapply(starships$results, as.data.frame)
starships$results
browseURL("https://github.com/ropensci/opendata")
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
ip_df <- geolocate(c(NA, "10.0.1.1", "", "72.33.67.89", "dds.ec", " ", "search.twitter.com"), .progress=FALSE)
ip_df
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "spiegel.de", "search.twitter.com"), .progress=FALSE)
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "spiegel.de", "search.twitter.com"), .progress=FALSE)
?geolocate
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
url <- "http://ip-api.com/xml/"
read_xml(url)
?trimws
ip_parsed <- read_xml(url)
xml_find_all(ip_parsed, "//query")
data.frame(ip_parsed)
xmlToDataFrame(ip_parsed)
library(XML)
xmlToDataFrame(ip_parsed)
as_list(ip_parsed)
ip_list <- as_list(ip_parsed)
ip_list
str(ip_list)
unlist(ip_list)
ip_list %>% unlist %>% as.data.frame(stringsAsFactors = FALSE)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
url <- "http://ip-api.com/json/"
ip_parsed <- fromJSON(url)
url <- "http://ip-api.com/json"
ip_parsed <- fromJSON(url)
?fromJSON
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
?fromJSON
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed
ip_parsed <- jsonlite::fromJSON(url, simplifyDataFrame = TRUE, flatten = TRUE)
ip_parsed
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
install.packages("translate")
install.packages("gdap")
install.packages("diezeit")
library(diezeit)
api_key <- "f9bd321bca869ff43311a8e83706413b82a099da993a41560640"
?zeit_search
foo <- zeit_search(endpoint="content", query="bayreuth")
str(foo)
length(foo)
author <- zeit_search(endpoint="author", query="stefan*locke")
author <- zeit_search(endpoint="author", query="johanna*haag")
author <- zeit_search(endpoint="author", query="tobias*landwehr")
author <- zeit_search(endpoint="keyword", query="wiedervereinigung")
author <- zeit_search(endpoint="series", query="deutschlandkarte")
series <- zeit_search(endpoint="series", query="deutschlandkarte")
zeit_get("content", "3Ed7KYJOO2MXu5SQtnudQA")
meta <- zeit_get("content", "3Ed7KYJOO2MXu5SQtnudQA")
merkel <- zeit_search(endpoint="content", query="title:Merkel")
str(merkel)
unlist(merkel)
foo <- zeit_search(endpoint="content", query="bayreuth")
names(foo)
dat <- zeit_search(endpoint="content", query="bayreuth")
names(dat)
dat$matches
dat$matches %>% unlist
str(dat$matches)
dd  <-  as.data.frame(matrix(unlist(dat$matches), nrow=length(dat$matches(listHolder[1]))))
length(dat$matches(listHolder[1])
dd  <-  as.data.frame(matrix(unlist(dat$matches), nrow=length(dat$matches(length(dat$matches(listHolder[1])[1]))))
foo <- zeit_search(endpoint="series", query="bayreuth")
author <- zeit_search(endpoint="author", query="tobias*landwehr")
dd  <-  as.data.frame(matrix(unlist(dat$matches), nrow=length(dat$matches(length(dat$matches(listHolder[1])[1])))))
dd  <-  as.data.frame(matrix(unlist(dat$matches),
nrow=length(unlist(dat$matches[1]))))
View(dd)
do.call(rbind, dat$matches).
do.call(rbind, dat$matches)
dd <- do.call(rbind, dat$matches)
View(dat$matches)
View(dd)
View(dd)
dd$snippelt
dd$snippet
names(dd)
class(dd)
dd <- do.call(rbind, dat$matches) %>% as.data.frame(stringsAsFactors = FALSE)
View(dd)
dd$snippet
sapply(dd, class)
df= as.data.frame(t(as.data.frame(dat$matches)))
View()
View(df)
as.data.frame(dat$matches)
do.call(c, unlist(dat$matches, recursive=FALSE))
sapply(dd, as.vector)
dd <- do.call(rbind, dat$matches) %>% as.data.frame(stringsAsFactors = FALSE)
View(dd)
sapply(dd, class)
sapply(dd, unlist)
ddd <- sapply(dd, unlist)
class(ddd)
ddd <- sapply(dd, unlist) %>% as.data.frame(stringsAsFactors = FALSE)
sapply(ddd, class)
View(ddd)
dd <- do.call(rbind, dat$matches) %>% as.data.frame(stringsAsFactors = FALSE) %>% sapply(unlist) %>% as.data.frame(stringsAsFactors = FALSE)
View(dd)
dd$snippet
foo <- zeit_search(endpoint="content", query="merkel")
merkel$found
merkel <- zeit_search(endpoint="content", query='"Kennedy" AND release_date:[1960-01-01T00:00:00Z TO 1969-12-31T23:59:59.999Z]')
meta <- zeit_get("content", "3Ed7KYJOO2MXu5SQtnudQA")
meta <- zeit_get("content", "Genug Liebe für mehr als zwei")
meta <- zeit_get("content", query = "Und Recht und Freibier", fields = "title")
meta <- zeit_get("content", "Und Recht und Freibier", fields = "title")
zeit_get
meta <- zeit_get("content", id = "Und Recht und Freibier", fields = "title")
zeit_client(print = TRUE)
?zeit_get
?zeit_search
?zeit_get
meta <- zeit_get("content", "3Ed7KYJOO2MXu5SQtnudQA")
source("packages.r")
source("functions.r")
library(diezeit)
merkel <- zeit_search(endpoint="content", query="merkel")
browseURL("http://ip-api.com/")
title <- "Groundhog Day" %>% URLencode()
endpoint <- "http://www.omdbapi.com/?"
url <- paste0(endpoint, "t=", title, "&tomatoes=true")
omdb_res = GET(url)
res_list <- content(omdb_res, as =  "parsed")
res_list %>% unlist() %>% t() %>% data.frame(stringsAsFactors = FALSE)
browseURL("http://www.omdbapi.com/")
browseURL("http://openweathermap.org/current")
library(newsAPI)
NEWSAPI_KEY <- "dd893f503fc24af2b4a47889244eaf27"
## save to .Renviron file
cat(
paste0("NEWSAPI_KEY=", NEWSAPI_KEY),
append = TRUE,
fill = TRUE,
file = file.path("~", ".Renviron")
)
src <- get_sources(language = "en")
src
src <- get_sources(language = "de")
src
df <- lapply(src$id, get_articles)
?get_articles
src <- get_sources(language = "en")
df <- lapply(src$id, get_articles)
get_articles("espn")
get_articles
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df
df <- do.call("rbind", df)
View(df)
src <- get_sources(language = "de")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df <- do.call("rbind", df)
View(df)
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY, sortBy = "latest")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY, sortBy = "top")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df <- do.call("rbind", df)
View(df)
get_articles("bild", apiKey = NEWSAPI_KEY)
source("packages.r")
source("functions.r")
browseURL("http://httpbin.org")
GET("http://httpbin.org/headers")
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com",
`User-Agent` = R.Version()$version.string))
url_response <- GET("http://spiegel.de/schlagzeilen",
add_headers(From = "my@email.com"))
url_parsed <- url_response  %>% read_html()
url_parsed %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
?html_session
browseURL("https://www.google.com/robots.txt")
browseURL("http://www.nytimes.com/robots.txt")
library(robotstxt)
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
browseURL("http://httpbin.org")
source("packages.r")
GET("http://httpbin.org/headers")
GET("http://httpbin.org/headers")
R.Version()$version.string
GET("http://httpbin.org/headers", add_headers(`User-Agent` = R.Version()$version.string))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(Hello = "my@email.com"))
browseURL("https://www.google.com/robots.txt")
library(robotstxt)
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "http://facebook.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
?paths_allowed
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
source("packages.r")
source("functions.r")
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
browseURL("http://ip-api.com/")
library(ipapi)
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
system("java -version")
rD <- rsDriver()
source("packages.r")
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
browseURL("https://mkearney.github.io/rtweet/articles/auth.html")
library(rtweet)
install.packages("rtweet")
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
library(rtweet)
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
appname <- "TwitterToR"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
install.packages("httpuv")
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
q <- paste0("schulz,merkel,btw17,btw2017")
?stream_tweets
twitter_stream_ger <- stream_tweets(q = q, timeout = 30, token = twitter_token)
twitter_stream_ger
class(twitter_stream_ger)
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "btw17"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
filename
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename,
token = twitter_token)
rt <- parse_stream(filename)
names(rt)
head(rt)
View(rt)
users_data(rt) %>% head()
users_data(rt) %>% names()
# set up directory and JSON dump
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "btw17"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
60*60
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 3600,
file_name = filename,
token = twitter_token)
rt <- parse_stream(filename)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 3600,
file_name = filename,
language = "de",
token = twitter_token)
source("packages.r")
